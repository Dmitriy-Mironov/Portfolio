{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Общее-впечатление\" data-toc-modified-id=\"Общее-впечатление-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление</font></a></span></li><li><span><a href=\"#Общее-впечатление-(ревью-2)\" data-toc-modified-id=\"Общее-впечатление-(ревью-2)-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span><font color=\"orange\">Общее впечатление (ревью 2)</font></a></span></li></ul></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обучение-моделей-на-корпусе-с-TfidVectorizer\" data-toc-modified-id=\"Обучение-моделей-на-корпусе-с-TfidVectorizer-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Обучение моделей на корпусе с TfidVectorizer</a></span></li><li><span><a href=\"#Обучение-моделей-на-корпусе-с-CountVectorizer\" data-toc-modified-id=\"Обучение-моделей-на-корпусе-с-CountVectorizer-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Обучение моделей на корпусе с CountVectorizer</a></span></li><li><span><a href=\"#Тестирование-лучшей-модели-по-результатам-обучающей-выборке-на-тестовой\" data-toc-modified-id=\"Тестирование-лучшей-модели-по-результатам-обучающей-выборке-на-тестовой-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Тестирование лучшей модели по результатам обучающей выборке на тестовой</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем все необходимые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "from tqdm import notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RND = 1 #для фиксации рандомайзера(random_state=RND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0) # index_col - установили индекс датафрейма\n",
    "display(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 81.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на кол-во дубликатов\n",
    "print(df.duplicated().sum())\n",
    "# посмотрим на кол-во пропусков\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликатов и пропусков в данных нету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение позитивных и негативных комментариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df['toxic'].value_counts())\n",
    "df['toxic'].value_counts(normalize=True) # содержит значения относительно частоты встречаемых значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Предобработка данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для лемматизации, удаления всех лишних символов кроме латинских букв и для удаления лишних пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation Why the edits made under my userna...\n",
       "1    D aww He matches this background colour I m se...\n",
       "2    Hey man I m really not trying to edit war It s...\n",
       "3    More I can t make any real suggestions on impr...\n",
       "4    You sir are my hero Any chance you remember wh...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook.tqdm.pandas() \n",
    "def text_clear(text):\n",
    "    clear_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    clear_text = ' '.join(clear_text.split())\n",
    "    return clear_text\n",
    "\n",
    "df['text'] = df['text'].apply(text_clear)#очищенный текст\n",
    "df['text'].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 55s, sys: 5.79 s, total: 13min 1s\n",
      "Wall time: 14min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "notebook.tqdm.pandas() \n",
    "# загружаем пакет omw-1.4 библиотеки nltk \n",
    "# для корректной работы функции lemmatizer.lemmatize()\n",
    "nltk.download('omw-1.4')\n",
    "# загрузим инструмент маркировки из библиотеки nltk, \n",
    "# который потребуется для удаления прилагательных\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# объявляем класс WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# создадим функцию для лемматизации датасета\n",
    "def lemmatize_text(text):\n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))#токенизируем текст и возвращаем кортеж [токен, nltk_tag]\n",
    "    lemmatized_text = []\n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'a'))#лемматизация прилагательных\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'v'))#лемматизация глаголов\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'n'))#лемматизация существительных\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'r'))#лемматизация наречий\n",
    "        else:\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word))#если тэги не найдены, выполнить неспецифическую лемматизацию\n",
    "    return \" \".join(lemmatized_text)#возвращаем нетокенизированный текст\n",
    "\n",
    "# лемматизируем текст сообщений обучающего признака\n",
    "df['text'] = df['text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датафрейм на две выборки: обучающую и тестовую в соотношение 75%:25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119469,), (39823,), (119469,), (39823,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df['text']\n",
    "target = df['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, \n",
    "                                                                            test_size=0.25, random_state=RND)\n",
    "\n",
    "features_train.shape, features_test.shape, target_train.shape, target_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим stopwords в выборках: features_train и features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим мешки слов двумя способами, чтобы протестировать модели на разных вариантах данных и выяснить какой способ преобразования данных покажет наилучший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов для признаков методом TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((119469, 135333), (39823, 135333))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train_tfv = count_tf_idf.fit_transform(features_train)\n",
    "features_test_tfv = count_tf_idf.transform(features_test)\n",
    "features_train_tfv.shape, features_test_tfv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов для признаков методом CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "count_vec = CountVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119469, 135333), (39823, 135333))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_cv = count_vec.fit_transform(features_train)\n",
    "features_test_cv = count_vec.transform(features_test)\n",
    "features_train_cv.shape, features_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 65.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "* данные импортированы с удалением лишнего индекса при чтении\n",
    "* дубликатов, пропусков нету\n",
    "* в данных виден большой дисбаланс классов\n",
    "* проведена лемматизация данных и удаление лишних символов\n",
    "* данные поделены на две выборки: обучающую и тестовую\n",
    "* удалены стоп-слова из признаков\n",
    "* получены мешки слов в признаках двумя способами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(5, shuffle=True, random_state=RND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на корпусе с TfidVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.7431893022567043\n",
      "CPU times: user 6min 14s, sys: 7min 32s, total: 13min 46s\n",
      "Wall time: 17min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_lin_tfv = {\"class_weight\": ['balanced'], \n",
    "             \"max_iter\": [50, 100, 150, 200]}\n",
    "\n",
    "model_lin_tfv = LogisticRegression(random_state=RND)\n",
    "\n",
    "model_lin_rscv_tfv = RandomizedSearchCV(\n",
    "    model_lin_tfv, params_lin_tfv, cv=cv, scoring='f1', \n",
    "    random_state=RND, refit = True, n_jobs=-1)\n",
    "model_lin_rscv_tfv.fit(features_train_tfv, target_train)\n",
    "\n",
    "best_model_lin_tfv = model_lin_rscv_tfv.best_estimator_\n",
    "best_params_model_lin_tfv = model_lin_rscv_tfv.best_params_\n",
    "best_score_lin_tfv = abs(model_lin_rscv_tfv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_lin_tfv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.6231634466875364\n",
      "CPU times: user 9min 41s, sys: 0 ns, total: 9min 41s\n",
      "Wall time: 9min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_tree_tfv = {\n",
    "          \"max_depth\": [10, 30, 50],   \n",
    "          \"class_weight\": ['balanced']\n",
    "         }\n",
    "model_tree_tfv = DecisionTreeClassifier(random_state=RND)\n",
    "\n",
    "model_tree_rscv_tfv = RandomizedSearchCV(model_tree_tfv, params_tree_tfv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_tree_rscv_tfv.fit(features_train_tfv, target_train)\n",
    "\n",
    "best_model_tree_tfv = model_tree_rscv_tfv.best_estimator_\n",
    "best_params_model_tree_tfv = model_tree_rscv_tfv.best_params_\n",
    "best_score_tree_tfv = abs(model_tree_rscv_tfv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_tree_tfv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.3678696428988617\n",
      "CPU times: user 11min 21s, sys: 0 ns, total: 11min 21s\n",
      "Wall time: 11min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_forest_tfv = {\n",
    "          \"max_depth\": [5, 10, 15], \n",
    "          \"n_estimators\": [30, 60, 90],  \n",
    "          \"class_weight\": ['balanced']\n",
    "         }\n",
    "model_forest_tfv = RandomForestClassifier(random_state=RND)\n",
    "\n",
    "model_forest_rscv_tfv = RandomizedSearchCV(model_forest_tfv, params_forest_tfv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_forest_rscv_tfv.fit(features_train_tfv, target_train)\n",
    "\n",
    "best_model_forest_tfv = model_forest_rscv_tfv.best_estimator_\n",
    "best_params_model_forest_tfv = model_forest_rscv_tfv.best_params_\n",
    "best_score_forest_tfv = abs(model_forest_rscv_tfv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_forest_tfv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель CatBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.6828816112319964\n",
      "CPU times: user 33min 32s, sys: 1.74 s, total: 33min 33s\n",
      "Wall time: 34min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_cat_tfv = {\n",
    "          \"max_depth\": [1, 2, 3], \n",
    "          \"n_estimators\": [10, 20, 30],  \n",
    "          \"auto_class_weights\": ['Balanced']\n",
    "         }\n",
    "model_cat_tfv = CatBoostClassifier(logging_level = 'Silent', random_state=RND)\n",
    "\n",
    "model_cat_rscv_tfv = RandomizedSearchCV(model_cat_tfv, params_cat_tfv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_cat_rscv_tfv.fit(features_train_tfv, target_train)\n",
    "\n",
    "best_model_cat_tfv = model_cat_rscv_tfv.best_estimator_\n",
    "best_params_model_cat_tfv = model_cat_rscv_tfv.best_params_\n",
    "best_score_cat_tfv = abs(model_cat_rscv_tfv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_cat_tfv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей на корпусе с CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.7527597428380723\n",
      "CPU times: user 9min 24s, sys: 11min 49s, total: 21min 13s\n",
      "Wall time: 21min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_lin_cv = {\"class_weight\": ['balanced'], \n",
    "             \"max_iter\": [50, 100, 150, 200]}\n",
    "\n",
    "model_lin_cv = LogisticRegression(random_state=RND)\n",
    "\n",
    "model_lin_rscv_cv = RandomizedSearchCV(\n",
    "    model_lin_cv, params_lin_cv, cv=cv, scoring='f1', \n",
    "    random_state=RND, refit = True, n_jobs=-1)\n",
    "model_lin_rscv_cv.fit(features_train_cv, target_train)\n",
    "\n",
    "best_model_lin_cv = model_lin_rscv_cv.best_estimator_\n",
    "best_params_model_lin_cv = model_lin_rscv_cv.best_params_\n",
    "best_score_lin_cv = abs(model_lin_rscv_cv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_lin_cv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced', max_iter=200, random_state=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_iter': 200, 'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_model_lin_cv)\n",
    "best_params_model_lin_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.6559146056372608\n",
      "CPU times: user 6min 7s, sys: 0 ns, total: 6min 7s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_tree_cv = {\n",
    "          \"max_depth\": [10, 30, 50],   \n",
    "          \"class_weight\": ['balanced']\n",
    "         }\n",
    "model_tree_cv = DecisionTreeClassifier(random_state=RND)\n",
    "\n",
    "model_tree_rscv_cv = RandomizedSearchCV(model_tree_cv, params_tree_cv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_tree_rscv_cv.fit(features_train_cv, target_train)\n",
    "\n",
    "best_model_tree_cv = model_tree_rscv_cv.best_estimator_\n",
    "best_params_model_tree_cv = model_tree_rscv_cv.best_params_\n",
    "best_score_tree_cv = abs(model_tree_rscv_cv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_tree_cv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.36396434570334846\n",
      "CPU times: user 8min 24s, sys: 49.4 ms, total: 8min 24s\n",
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_forest_cv = {\n",
    "          \"max_depth\": [5, 10, 15], \n",
    "          \"n_estimators\": [30, 60, 90],  \n",
    "          \"class_weight\": ['balanced']\n",
    "         }\n",
    "model_forest_cv = RandomForestClassifier(random_state=RND)\n",
    "\n",
    "model_forest_rscv_cv = RandomizedSearchCV(model_forest_cv, params_forest_cv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_forest_rscv_cv.fit(features_train_cv, target_train)\n",
    "\n",
    "best_model_forest_cv = model_forest_rscv_cv.best_estimator_\n",
    "best_params_model_forest_cv = model_forest_rscv_cv.best_params_\n",
    "best_score_forest_cv = abs(model_forest_rscv_cv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_forest_cv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель CatBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на train = 0.6855539429272997\n",
      "CPU times: user 27min 11s, sys: 14.8 s, total: 27min 25s\n",
      "Wall time: 28min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_cat_cv = {\n",
    "          \"max_depth\": [1, 2, 3], \n",
    "          \"n_estimators\": [10, 20, 30],  \n",
    "          \"auto_class_weights\": ['Balanced']\n",
    "         }\n",
    "model_cat_cv = CatBoostClassifier(logging_level = 'Silent', random_state=RND)\n",
    "\n",
    "model_cat_rscv_cv = RandomizedSearchCV(model_cat_cv, params_cat_cv, cv=cv, scoring='f1',\n",
    "                                       random_state=RND, refit = True, n_jobs=-1)\n",
    "model_cat_rscv_cv.fit(features_train_cv, target_train)\n",
    "\n",
    "best_model_cat_cv = model_cat_rscv_cv.best_estimator_\n",
    "best_params_model_cat_cv = model_cat_rscv_cv.best_params_\n",
    "best_score_cat_cv = abs(model_cat_rscv_cv.best_score_)\n",
    "\n",
    "print(f'F1 на train = {best_score_cat_cv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лучший результат на обучающей выборке показала модель LogisticRegression с гиперпараметрами: (class_weight='balanced', max_iter=200, random_state=1) на корпусе обработанным методом CountVectorizer, её и будем тестировать на тестовой выборке**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование лучшей модели по результатам обучающей выборке на тестовой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = best_model_lin_cv.predict(features_test_cv)\n",
    "best_test_score = round(f1_score(target_test, predict), 2)\n",
    "best_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка на адекватность модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_model = DummyClassifier(strategy='uniform', random_state=RND)\n",
    "dummy_model.fit(features_train_cv, target_train)\n",
    "dummy_predict = dummy_model.predict(features_test_cv)\n",
    "dummy_score = round(f1_score(target_test, dummy_predict), 2)\n",
    "dummy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель прошла проверку на адекватность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 на обучающей выборке</th>\n",
       "      <th>F1 на тестовой выборке</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_CV</th>\n",
       "      <td>0.75276</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_Tfidf</th>\n",
       "      <td>0.743189</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier_CV</th>\n",
       "      <td>0.655915</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier_Tfidf</th>\n",
       "      <td>0.623163</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier_CV</th>\n",
       "      <td>0.363964</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier_Tfidf</th>\n",
       "      <td>0.36787</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier_CV</th>\n",
       "      <td>0.685554</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier_Tfidf</th>\n",
       "      <td>0.682882</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td></td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             F1 на обучающей выборке F1 на тестовой выборке\n",
       "LogisticRegression_CV                        0.75276                   0.75\n",
       "LogisticRegression_Tfidf                    0.743189                       \n",
       "DecisionTreeClassifier_CV                   0.655915                       \n",
       "DecisionTreeClassifier_Tfidf                0.623163                       \n",
       "RandomForestClassifier_CV                   0.363964                       \n",
       "RandomForestClassifier_Tfidf                 0.36787                       \n",
       "CatBoostClassifier_CV                       0.685554                       \n",
       "CatBoostClassifier_Tfidf                    0.682882                       \n",
       "DummyClassifier                                                        0.17"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rezult = pd.DataFrame(index=['F1 на обучающей выборке', 'F1 на тестовой выборке'],\n",
    "                      columns=['LogisticRegression_CV', 'LogisticRegression_Tfidf', 'DecisionTreeClassifier_CV',\n",
    "                      'DecisionTreeClassifier_Tfidf', 'RandomForestClassifier_CV', 'RandomForestClassifier_Tfidf',\n",
    "                      'CatBoostClassifier_CV', 'CatBoostClassifier_Tfidf', 'DummyClassifier'])\n",
    "rezult['LogisticRegression_CV'] = best_score_lin_cv, best_test_score\n",
    "rezult['DecisionTreeClassifier_CV'] = best_score_tree_cv, ''\n",
    "rezult['RandomForestClassifier_CV'] = best_score_forest_cv, ''\n",
    "rezult['CatBoostClassifier_CV'] = best_score_cat_cv, ''\n",
    "rezult['LogisticRegression_Tfidf'] = best_score_lin_tfv, ''\n",
    "rezult['DecisionTreeClassifier_Tfidf'] = best_score_tree_tfv, ''\n",
    "rezult['RandomForestClassifier_Tfidf'] = best_score_forest_tfv, ''\n",
    "rezult['CatBoostClassifier_Tfidf'] = best_score_cat_tfv, ''\n",
    "rezult['DummyClassifier'] = '', dummy_score\n",
    "rezult.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **На этапе знакомства с данными и их предобработкой были подведены такие итоги:**\n",
    "    * данные импортированы с удалением лишнего индекса при чтении\n",
    "    * дубликатов, пропусков нету\n",
    "    * в данных виден большой дисбаланс классов\n",
    "    * проведена лемматизация данных и удаление лишних символов\n",
    "    * данные поделены на две выборки: обучающую и тестовую\n",
    "    * удалены стоп-слова из признаков\n",
    "    * получены мешки слов в признаках двумя способами\n",
    "* **На этапе обучения моделей:**\n",
    "    * обучены 4 модели на двух разных мешках признаков, полученными методами CountVectorizer и TfidVectorizer\n",
    "    * лучший результат на обучающей выборке показала модель LogisticRegression с гиперпараметрами: class_weight='balanced', max_iter=200, random_state=1 на признаках полученными методом CountVectorizer\n",
    "    * на тестовой выборке лучшая модель показала метрику F1 = 0.75\n",
    "    * проверили модель на адекватность, модель DummyClassifier показала метрику F1 = 0.17 - это означает наша модель работает правильно, так как если сопоставить точность предсказаний модели LogisticRegression с точностью предсказаний модели DummyClassifier, то результат нашей подобранной модели значительно выше результата Dummy модели\n",
    "* **Для выполнения поставленной задачи интернет-магазина «Викишоп» подходит модель *LogisticRegression.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1305,
    "start_time": "2023-01-26T18:47:23.391Z"
   },
   {
    "duration": 2492,
    "start_time": "2023-01-26T18:47:27.246Z"
   },
   {
    "duration": 947,
    "start_time": "2023-01-26T18:50:55.446Z"
   },
   {
    "duration": 201,
    "start_time": "2023-01-26T18:51:45.988Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-26T18:54:08.898Z"
   },
   {
    "duration": 12,
    "start_time": "2023-01-26T18:55:18.095Z"
   },
   {
    "duration": 16,
    "start_time": "2023-01-26T18:58:07.870Z"
   },
   {
    "duration": 220,
    "start_time": "2023-01-26T19:02:13.533Z"
   },
   {
    "duration": 88248,
    "start_time": "2023-01-26T20:16:02.036Z"
   },
   {
    "duration": 1350,
    "start_time": "2023-01-27T14:17:37.578Z"
   },
   {
    "duration": 3799,
    "start_time": "2023-01-27T14:17:38.929Z"
   },
   {
    "duration": 54,
    "start_time": "2023-01-27T14:17:42.730Z"
   },
   {
    "duration": 237,
    "start_time": "2023-01-27T14:17:42.786Z"
   },
   {
    "duration": 10,
    "start_time": "2023-01-27T14:17:43.025Z"
   },
   {
    "duration": 91131,
    "start_time": "2023-01-27T14:17:43.036Z"
   },
   {
    "duration": 117,
    "start_time": "2023-01-27T14:19:14.169Z"
   },
   {
    "duration": 894,
    "start_time": "2023-01-27T14:19:14.288Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-27T14:19:15.184Z"
   },
   {
    "duration": 34,
    "start_time": "2023-01-27T14:21:04.186Z"
   },
   {
    "duration": 6563,
    "start_time": "2023-01-27T14:21:11.323Z"
   },
   {
    "duration": 117,
    "start_time": "2023-01-27T14:21:23.426Z"
   },
   {
    "duration": 38,
    "start_time": "2023-01-27T14:52:55.044Z"
   },
   {
    "duration": 1372,
    "start_time": "2023-01-27T14:55:19.199Z"
   },
   {
    "duration": 934,
    "start_time": "2023-01-27T14:55:20.573Z"
   },
   {
    "duration": 70,
    "start_time": "2023-01-27T14:55:21.509Z"
   },
   {
    "duration": 251,
    "start_time": "2023-01-27T14:55:21.581Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-27T14:55:21.834Z"
   },
   {
    "duration": 89870,
    "start_time": "2023-01-27T14:55:21.856Z"
   },
   {
    "duration": 48,
    "start_time": "2023-01-27T14:56:51.728Z"
   },
   {
    "duration": 6487,
    "start_time": "2023-01-27T14:56:51.777Z"
   },
   {
    "duration": 126,
    "start_time": "2023-01-27T14:56:58.266Z"
   },
   {
    "duration": 37,
    "start_time": "2023-01-27T14:56:58.394Z"
   },
   {
    "duration": 1389,
    "start_time": "2023-01-27T14:57:34.270Z"
   },
   {
    "duration": 943,
    "start_time": "2023-01-27T14:57:35.660Z"
   },
   {
    "duration": 35,
    "start_time": "2023-01-27T14:57:36.605Z"
   },
   {
    "duration": 70,
    "start_time": "2023-01-27T14:57:36.641Z"
   },
   {
    "duration": 259,
    "start_time": "2023-01-27T14:57:36.713Z"
   },
   {
    "duration": 11,
    "start_time": "2023-01-27T14:57:36.974Z"
   },
   {
    "duration": 89899,
    "start_time": "2023-01-27T14:57:36.986Z"
   },
   {
    "duration": 32,
    "start_time": "2023-01-27T14:59:06.887Z"
   },
   {
    "duration": 6451,
    "start_time": "2023-01-27T14:59:06.920Z"
   },
   {
    "duration": 120,
    "start_time": "2023-01-27T14:59:13.373Z"
   },
   {
    "duration": 46,
    "start_time": "2023-01-27T14:59:13.496Z"
   },
   {
    "duration": 30,
    "start_time": "2023-01-27T19:25:51.444Z"
   },
   {
    "duration": 121,
    "start_time": "2023-01-27T19:25:52.246Z"
   },
   {
    "duration": 1535,
    "start_time": "2023-01-27T19:26:09.670Z"
   },
   {
    "duration": 967,
    "start_time": "2023-01-27T19:26:11.207Z"
   },
   {
    "duration": 34,
    "start_time": "2023-01-27T19:26:12.175Z"
   },
   {
    "duration": 73,
    "start_time": "2023-01-27T19:26:12.212Z"
   },
   {
    "duration": 254,
    "start_time": "2023-01-27T19:26:12.287Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-27T19:26:12.542Z"
   },
   {
    "duration": 101026,
    "start_time": "2023-01-27T19:26:12.563Z"
   },
   {
    "duration": 49,
    "start_time": "2023-01-27T19:27:53.591Z"
   },
   {
    "duration": 6717,
    "start_time": "2023-01-27T19:27:53.642Z"
   },
   {
    "duration": 120,
    "start_time": "2023-01-27T19:28:00.361Z"
   },
   {
    "duration": 36,
    "start_time": "2023-01-27T19:28:00.483Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-27T19:41:00.681Z"
   },
   {
    "duration": 30,
    "start_time": "2023-01-27T19:41:01.417Z"
   },
   {
    "duration": 166,
    "start_time": "2023-01-27T19:41:14.715Z"
   },
   {
    "duration": 735397,
    "start_time": "2023-01-27T19:42:39.366Z"
   },
   {
    "duration": 728394,
    "start_time": "2023-01-27T20:18:22.960Z"
   },
   {
    "duration": 29,
    "start_time": "2023-01-27T20:32:17.654Z"
   },
   {
    "duration": 84,
    "start_time": "2023-01-27T20:42:11.943Z"
   },
   {
    "duration": 92,
    "start_time": "2023-01-27T20:42:26.287Z"
   },
   {
    "duration": 97,
    "start_time": "2023-01-27T20:42:55.444Z"
   },
   {
    "duration": 100,
    "start_time": "2023-01-27T20:43:09.028Z"
   },
   {
    "duration": 115,
    "start_time": "2023-01-28T06:36:58.080Z"
   },
   {
    "duration": 1360,
    "start_time": "2023-01-28T06:37:08.885Z"
   },
   {
    "duration": 3430,
    "start_time": "2023-01-28T06:37:10.247Z"
   },
   {
    "duration": 38,
    "start_time": "2023-01-28T06:37:13.680Z"
   },
   {
    "duration": 62,
    "start_time": "2023-01-28T06:37:13.720Z"
   },
   {
    "duration": 266,
    "start_time": "2023-01-28T06:37:13.798Z"
   },
   {
    "duration": 17,
    "start_time": "2023-01-28T06:37:14.066Z"
   },
   {
    "duration": 115854,
    "start_time": "2023-01-28T06:37:14.098Z"
   },
   {
    "duration": 44,
    "start_time": "2023-01-28T06:39:09.954Z"
   },
   {
    "duration": 7453,
    "start_time": "2023-01-28T06:39:10.000Z"
   },
   {
    "duration": 122,
    "start_time": "2023-01-28T06:39:17.454Z"
   },
   {
    "duration": 47,
    "start_time": "2023-01-28T06:39:17.578Z"
   },
   {
    "duration": 23,
    "start_time": "2023-01-28T06:39:17.627Z"
   },
   {
    "duration": 14087,
    "start_time": "2023-01-28T06:39:17.652Z"
   },
   {
    "duration": 78,
    "start_time": "2023-01-28T06:39:31.742Z"
   },
   {
    "duration": 15,
    "start_time": "2023-01-28T06:39:31.822Z"
   },
   {
    "duration": 744817,
    "start_time": "2023-01-28T06:39:54.290Z"
   },
   {
    "duration": 165,
    "start_time": "2023-01-28T06:52:19.111Z"
   },
   {
    "duration": 18,
    "start_time": "2023-01-28T06:52:19.278Z"
   },
   {
    "duration": 459485,
    "start_time": "2023-01-28T06:56:52.059Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-28T07:09:41.382Z"
   },
   {
    "duration": 52,
    "start_time": "2023-01-28T08:23:51.926Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-28T08:24:00.169Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-28T08:24:04.144Z"
   },
   {
    "duration": 12693,
    "start_time": "2023-01-28T08:24:05.030Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-28T08:24:20.531Z"
   },
   {
    "duration": 419157,
    "start_time": "2023-01-28T08:24:23.501Z"
   },
   {
    "duration": 27668,
    "start_time": "2023-01-28T08:34:18.900Z"
   },
   {
    "duration": 1975467,
    "start_time": "2023-01-28T08:35:35.486Z"
   },
   {
    "duration": 1443,
    "start_time": "2023-01-28T09:25:46.530Z"
   },
   {
    "duration": 3262,
    "start_time": "2023-01-28T09:25:47.976Z"
   },
   {
    "duration": 33,
    "start_time": "2023-01-28T09:25:51.240Z"
   },
   {
    "duration": 77,
    "start_time": "2023-01-28T09:25:51.274Z"
   },
   {
    "duration": 262,
    "start_time": "2023-01-28T09:25:51.354Z"
   },
   {
    "duration": 22,
    "start_time": "2023-01-28T09:25:51.618Z"
   },
   {
    "duration": 101021,
    "start_time": "2023-01-28T09:25:51.642Z"
   },
   {
    "duration": 147,
    "start_time": "2023-01-28T09:27:32.665Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.818Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.818Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.819Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.820Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.821Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.821Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.823Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.823Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.826Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.827Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.828Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.829Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.830Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.831Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:27:32.832Z"
   },
   {
    "duration": 52,
    "start_time": "2023-01-28T09:33:43.873Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-28T09:33:44.562Z"
   },
   {
    "duration": 7376,
    "start_time": "2023-01-28T09:33:44.999Z"
   },
   {
    "duration": 18,
    "start_time": "2023-01-28T09:33:52.377Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.404Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.405Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.406Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.407Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.407Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.408Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.409Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.410Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.410Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.411Z"
   },
   {
    "duration": 0,
    "start_time": "2023-01-28T09:33:52.412Z"
   },
   {
    "duration": 9,
    "start_time": "2023-01-28T09:33:52.942Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-28T09:37:00.369Z"
   },
   {
    "duration": 26,
    "start_time": "2023-01-28T09:37:06.666Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-28T09:51:04.438Z"
   },
   {
    "duration": 16,
    "start_time": "2023-01-28T09:51:07.164Z"
   },
   {
    "duration": 15,
    "start_time": "2023-01-28T09:51:44.387Z"
   },
   {
    "duration": 1432,
    "start_time": "2023-01-28T09:53:06.287Z"
   },
   {
    "duration": 1472,
    "start_time": "2023-01-28T09:53:07.725Z"
   },
   {
    "duration": 40,
    "start_time": "2023-01-28T09:53:09.200Z"
   },
   {
    "duration": 77,
    "start_time": "2023-01-28T09:53:09.241Z"
   },
   {
    "duration": 244,
    "start_time": "2023-01-28T09:53:09.323Z"
   },
   {
    "duration": 11,
    "start_time": "2023-01-28T09:53:09.569Z"
   },
   {
    "duration": 104297,
    "start_time": "2023-01-28T09:53:09.581Z"
   },
   {
    "duration": 51,
    "start_time": "2023-01-28T09:54:53.880Z"
   },
   {
    "duration": 6,
    "start_time": "2023-01-28T09:54:53.934Z"
   },
   {
    "duration": 7941,
    "start_time": "2023-01-28T09:54:53.944Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-28T09:55:01.887Z"
   },
   {
    "duration": 7875,
    "start_time": "2023-01-28T09:55:01.902Z"
   },
   {
    "duration": 140,
    "start_time": "2023-01-28T09:55:09.779Z"
   },
   {
    "duration": 38,
    "start_time": "2023-01-28T09:55:09.924Z"
   },
   {
    "duration": 26,
    "start_time": "2023-01-28T09:55:09.964Z"
   },
   {
    "duration": 214,
    "start_time": "2023-01-28T09:55:09.991Z"
   },
   {
    "duration": 56,
    "start_time": "2023-01-28T09:55:10.210Z"
   },
   {
    "duration": 74,
    "start_time": "2023-01-28T09:55:10.267Z"
   },
   {
    "duration": 53,
    "start_time": "2023-01-28T09:55:10.343Z"
   },
   {
    "duration": 69,
    "start_time": "2023-01-28T09:55:10.397Z"
   },
   {
    "duration": 74,
    "start_time": "2023-01-28T09:55:10.467Z"
   },
   {
    "duration": 54,
    "start_time": "2023-01-28T09:55:10.543Z"
   },
   {
    "duration": 72,
    "start_time": "2023-01-28T09:55:10.598Z"
   },
   {
    "duration": 10,
    "start_time": "2023-01-28T09:55:10.671Z"
   },
   {
    "duration": 935110,
    "start_time": "2023-01-28T09:58:40.792Z"
   },
   {
    "duration": 400632,
    "start_time": "2023-01-28T10:14:15.904Z"
   },
   {
    "duration": 470976,
    "start_time": "2023-01-28T10:20:56.537Z"
   },
   {
    "duration": 1923802,
    "start_time": "2023-01-28T10:28:47.515Z"
   },
   {
    "duration": 1375008,
    "start_time": "2023-01-28T11:00:51.318Z"
   },
   {
    "duration": 375494,
    "start_time": "2023-01-28T11:23:46.334Z"
   },
   {
    "duration": 461862,
    "start_time": "2023-01-28T11:30:01.830Z"
   },
   {
    "duration": 1655797,
    "start_time": "2023-01-28T11:37:43.694Z"
   },
   {
    "duration": 28,
    "start_time": "2023-01-28T12:05:19.493Z"
   },
   {
    "duration": 19,
    "start_time": "2023-01-28T13:05:22.821Z"
   },
   {
    "duration": 18,
    "start_time": "2023-01-28T13:05:51.237Z"
   },
   {
    "duration": 23,
    "start_time": "2023-01-28T13:06:01.181Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-28T13:11:09.508Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-28T13:30:33.599Z"
   },
   {
    "duration": 22,
    "start_time": "2023-01-28T13:59:01.622Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-28T14:01:34.064Z"
   },
   {
    "duration": 16,
    "start_time": "2023-01-28T14:01:47.787Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-28T14:30:31.420Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-28T14:30:56.258Z"
   },
   {
    "duration": 5,
    "start_time": "2023-01-28T14:31:14.857Z"
   },
   {
    "duration": 62,
    "start_time": "2023-01-28T14:52:00.289Z"
   },
   {
    "duration": 11,
    "start_time": "2023-01-28T15:13:58.449Z"
   },
   {
    "duration": 21,
    "start_time": "2023-01-28T15:14:17.002Z"
   },
   {
    "duration": 26,
    "start_time": "2023-01-28T15:14:29.089Z"
   },
   {
    "duration": 17,
    "start_time": "2023-01-28T15:47:03.421Z"
   },
   {
    "duration": 1308,
    "start_time": "2023-01-30T13:53:54.750Z"
   },
   {
    "duration": 3491,
    "start_time": "2023-01-30T13:53:56.060Z"
   },
   {
    "duration": 57,
    "start_time": "2023-01-30T13:53:59.552Z"
   },
   {
    "duration": 255,
    "start_time": "2023-01-30T13:53:59.612Z"
   },
   {
    "duration": 13,
    "start_time": "2023-01-30T13:53:59.869Z"
   },
   {
    "duration": 4102,
    "start_time": "2023-01-30T13:53:59.884Z"
   },
   {
    "duration": 1485,
    "start_time": "2023-01-30T13:54:03.988Z"
   },
   {
    "duration": 32,
    "start_time": "2023-01-30T13:54:05.474Z"
   },
   {
    "duration": 10,
    "start_time": "2023-01-30T13:54:05.508Z"
   },
   {
    "duration": 7133,
    "start_time": "2023-01-30T13:54:05.521Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T13:54:12.656Z"
   },
   {
    "duration": 7077,
    "start_time": "2023-01-30T13:54:12.662Z"
   },
   {
    "duration": 78,
    "start_time": "2023-01-30T13:54:19.741Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T13:54:19.820Z"
   },
   {
    "duration": 1350,
    "start_time": "2023-01-30T13:55:08.641Z"
   },
   {
    "duration": 890,
    "start_time": "2023-01-30T13:55:09.993Z"
   },
   {
    "duration": 69,
    "start_time": "2023-01-30T13:55:10.885Z"
   },
   {
    "duration": 270,
    "start_time": "2023-01-30T13:55:10.956Z"
   },
   {
    "duration": 14,
    "start_time": "2023-01-30T13:55:11.228Z"
   },
   {
    "duration": 4097,
    "start_time": "2023-01-30T13:55:11.244Z"
   },
   {
    "duration": 423,
    "start_time": "2023-01-30T13:55:15.342Z"
   },
   {
    "duration": 35,
    "start_time": "2023-01-30T13:55:15.766Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T13:55:15.803Z"
   },
   {
    "duration": 7129,
    "start_time": "2023-01-30T13:55:15.810Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T13:55:22.940Z"
   },
   {
    "duration": 7222,
    "start_time": "2023-01-30T13:55:22.946Z"
   },
   {
    "duration": 74,
    "start_time": "2023-01-30T13:55:30.169Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-30T13:55:30.245Z"
   },
   {
    "duration": 1489,
    "start_time": "2023-01-30T13:55:52.979Z"
   },
   {
    "duration": 889,
    "start_time": "2023-01-30T13:55:54.470Z"
   },
   {
    "duration": 73,
    "start_time": "2023-01-30T13:55:55.361Z"
   },
   {
    "duration": 269,
    "start_time": "2023-01-30T13:55:55.436Z"
   },
   {
    "duration": 13,
    "start_time": "2023-01-30T13:55:55.707Z"
   },
   {
    "duration": 4081,
    "start_time": "2023-01-30T13:55:55.722Z"
   },
   {
    "duration": 827,
    "start_time": "2023-01-30T13:55:59.805Z"
   },
   {
    "duration": 36,
    "start_time": "2023-01-30T13:56:00.634Z"
   },
   {
    "duration": 36,
    "start_time": "2023-01-30T13:56:00.672Z"
   },
   {
    "duration": 7071,
    "start_time": "2023-01-30T13:56:00.712Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T13:56:07.784Z"
   },
   {
    "duration": 6999,
    "start_time": "2023-01-30T13:56:07.790Z"
   },
   {
    "duration": 72,
    "start_time": "2023-01-30T13:56:14.791Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-30T13:56:14.865Z"
   },
   {
    "duration": 1421,
    "start_time": "2023-01-30T13:59:23.422Z"
   },
   {
    "duration": 895,
    "start_time": "2023-01-30T13:59:24.845Z"
   },
   {
    "duration": 71,
    "start_time": "2023-01-30T13:59:25.742Z"
   },
   {
    "duration": 267,
    "start_time": "2023-01-30T13:59:25.815Z"
   },
   {
    "duration": 20,
    "start_time": "2023-01-30T13:59:26.084Z"
   },
   {
    "duration": 3939,
    "start_time": "2023-01-30T13:59:26.105Z"
   },
   {
    "duration": 514724,
    "start_time": "2023-01-30T13:59:30.046Z"
   },
   {
    "duration": 45,
    "start_time": "2023-01-30T14:08:04.773Z"
   },
   {
    "duration": 3,
    "start_time": "2023-01-30T14:08:04.820Z"
   },
   {
    "duration": 6643,
    "start_time": "2023-01-30T14:08:04.826Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T14:08:11.471Z"
   },
   {
    "duration": 6896,
    "start_time": "2023-01-30T14:08:11.477Z"
   },
   {
    "duration": 67,
    "start_time": "2023-01-30T14:08:18.375Z"
   },
   {
    "duration": 4,
    "start_time": "2023-01-30T14:08:18.444Z"
   },
   {
    "duration": 719357,
    "start_time": "2023-01-30T14:08:18.450Z"
   },
   {
    "duration": 419017,
    "start_time": "2023-01-30T14:20:17.809Z"
   },
   {
    "duration": 470606,
    "start_time": "2023-01-30T14:27:16.828Z"
   },
   {
    "duration": 164,
    "start_time": "2023-01-30T16:14:17.665Z"
   },
   {
    "duration": 2757,
    "start_time": "2023-01-30T16:14:27.868Z"
   },
   {
    "duration": 4694,
    "start_time": "2023-01-30T16:14:30.628Z"
   },
   {
    "duration": 108,
    "start_time": "2023-01-30T16:14:35.329Z"
   },
   {
    "duration": 431,
    "start_time": "2023-01-30T16:14:35.444Z"
   },
   {
    "duration": 17,
    "start_time": "2023-01-30T16:14:35.880Z"
   },
   {
    "duration": 7106,
    "start_time": "2023-01-30T16:14:35.899Z"
   },
   {
    "duration": 892450,
    "start_time": "2023-01-30T16:14:43.009Z"
   },
   {
    "duration": 182,
    "start_time": "2023-01-30T16:29:35.477Z"
   },
   {
    "duration": 87,
    "start_time": "2023-01-30T16:29:35.672Z"
   },
   {
    "duration": 15943,
    "start_time": "2023-01-30T16:29:35.768Z"
   },
   {
    "duration": 11,
    "start_time": "2023-01-30T16:29:51.714Z"
   },
   {
    "duration": 14658,
    "start_time": "2023-01-30T16:29:51.727Z"
   },
   {
    "duration": 110,
    "start_time": "2023-01-30T16:30:06.393Z"
   },
   {
    "duration": 102,
    "start_time": "2023-01-30T16:30:06.508Z"
   },
   {
    "duration": 1033195,
    "start_time": "2023-01-30T16:30:06.613Z"
   },
   {
    "duration": 597929,
    "start_time": "2023-01-30T16:47:19.815Z"
   },
   {
    "duration": 690294,
    "start_time": "2023-01-30T16:57:17.752Z"
   },
   {
    "duration": 2059399,
    "start_time": "2023-01-30T17:08:48.048Z"
   },
   {
    "duration": 1275302,
    "start_time": "2023-01-30T17:43:07.449Z"
   },
   {
    "duration": 15,
    "start_time": "2023-01-30T18:04:22.753Z"
   },
   {
    "duration": 368174,
    "start_time": "2023-01-30T18:04:22.775Z"
   },
   {
    "duration": 505221,
    "start_time": "2023-01-30T18:10:30.952Z"
   },
   {
    "duration": 1683386,
    "start_time": "2023-01-30T18:18:56.175Z"
   },
   {
    "duration": 27,
    "start_time": "2023-01-30T18:46:59.563Z"
   },
   {
    "duration": 21,
    "start_time": "2023-01-30T18:46:59.592Z"
   },
   {
    "duration": 13,
    "start_time": "2023-01-30T18:46:59.615Z"
   },
   {
    "duration": 30,
    "start_time": "2023-01-30T18:51:04.835Z"
   },
   {
    "duration": 30,
    "start_time": "2023-01-30T18:51:15.310Z"
   },
   {
    "duration": 35,
    "start_time": "2023-01-30T19:03:06.706Z"
   },
   {
    "duration": 21,
    "start_time": "2023-01-30T19:05:39.401Z"
   },
   {
    "duration": 18,
    "start_time": "2023-01-30T19:05:43.394Z"
   },
   {
    "duration": 8,
    "start_time": "2023-01-30T19:10:56.710Z"
   },
   {
    "duration": 123,
    "start_time": "2023-01-30T19:11:05.859Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
